# CVPR-2024-Speech_Audio_Music-Papers
A collections of papers related to the topics of joint learning of vision with speech, audio or music (audio-visual learning) in CVPR 2024. For folks who work in the field related to audio-visual learning, computer audition, speech/audio/music, this repo gives a brief summary of the accepted papers (main conference) along with their code and/or dataset. More information will be updated for works that are accepted in workshops or demos.

## üóûÔ∏è papers along with associated code and/or dataset.

## Speech

* [Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion](https://openaccess.thecvf.com/content/CVPR2024/papers/Chhatre_Emotional_Speech-driven_3D_Body_Animation_via_Disentangled_Latent_Diffusion_CVPR_2024_paper.pdf) Chhatre, Kiran and Danƒõƒçek, Radek and Athanasiou, Nikos and Becherini, Giorgio and Peters, Christopher and Black, Michael J. and Bolkart, Timo [[code]](https://github.com/kiranchhatre/amuse)
* [ES¬≥: Evolving Self-Supervised Learning of Robust Audio-Visual Speech Representations](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_ES3_Evolving_Self-Supervised_Learning_of_Robust_Audio-Visual_Speech_Representations_CVPR_2024_paper.pdf) Yuanhang, Zhang and Shuang, Yang and Shiguang, Shan and Xilin, Chen
* [Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks Methods and Applications](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Probabilistic_Speech-Driven_3D_Facial_Motion_Synthesis_New_Benchmarks_Methods_and_CVPR_2024_paper.pdf) Karren D. Yang and Anurag, Ranjan and Jen-Hao Rick Chang and Raviteja, Vemulapalli and Oncel, Tuzel
* [DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_DiffSHEG_A_Diffusion-Based_Approach_for_Real-Time_Speech-driven_Holistic_3D_Expression_CVPR_2024_paper.pdf) Junming Chen, Yunfei Liu, Jianan, Wang and Ailing, Zeng and Yu, Li and Qifeng, Chen [[code]](https://github.com/JeremyCJM/DiffSHEG)
* [Faces that Speak: Jointly Synthesising Talking Face and Speech from Text](https://openaccess.thecvf.com/content/CVPR2024/papers/Jang_Faces_that_Speak_Jointly_Synthesising_Talking_Face_and_Speech_from_CVPR_2024_paper.pdf)  Youngjoon, Jang and Ji-Hoon, Kim and Junseok, Ahn and Doyeop, Kwak and Hong-Sun, Yang and Yoon-Cheol, Ju and Il-Hwan, Kim and Byeong-Yeol, Kim and Joon Son Chung
* [Towards Variable and Coordinated Holistic Co-Speech Motion Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Towards_Variable_and_Coordinated_Holistic_Co-Speech_Motion_Generation_CVPR_2024_paper.pdf) Yifei Liu, Qiong Cao, Yandong Wen, Huaiguang Jiang, Changxing Ding [[code]](https://github.com/feifeifeiliu/ProbTalk)
* [ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis](https://openaccess.thecvf.com/content/CVPR2024/papers/Mughal_ConvoFusion_Multi-Modal_Conversational_Diffusion_for_Co-Speech_Gesture_Synthesis_CVPR_2024_paper.pdf) Muhammad Hamza Mughal and Rishabh, Dabral and Ikhsanul, Habibie and Lucia Donatelli and Marc, Habermann and Christian Theobalt [[code]](https://github.com/m-hamza-mughal/convofusion)
* [A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition](https://openaccess.thecvf.com/content/CVPR2024/papers/Dai_A_Study_of_Dropout-Induced_Modality_Bias_on_Robustness_to_Missing_CVPR_2024_paper.pdf) Yusheng, Dai and Hang, Chen and Jun, Du and Xiaofei, Ding and Ning, Ding and Feijun, Jiang and Chin-Hui, Lee [[code]](https://github.com/dalision/ModalBiasAVSR)
* [AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation](https://openaccess.thecvf.com/content/CVPR2024/papers/Choi_AV2AV_Direct_Audio-Visual_Speech_to_Audio-Visual_Speech_Translation_with_Unified_CVPR_2024_paper.pdf) Jeongsoo, Choi and Se Jin Park and Minsu, Kim and Yong Man Ro [[code]](https://github.com/choijeongsoo/av2av), CVPR Highlight
* [Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model](https://openaccess.thecvf.com/content/CVPR2024/papers/He_Co-Speech_Gesture_Video_Generation_via_Motion-Decoupled_Diffusion_Model_CVPR_2024_paper.pdf) He, Xu and Huang, Qiaochu and Zhang, Zhensong and Lin, Zhiwei and Wu, Zhiyong and Yang, Sicheng and Li, Minglei and Chen, Zhiyi and Xu, Songcen and Wu, Xiaofei [[code]](https://github.com/thuhcsi/S2G-MDDiffusion)
* [Learning Adaptive Spatial Coherent Correlations for Speech-Preserving Facial Expression Manipulation](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Learning_Adaptive_Spatial_Coherent_Correlations_for_Speech-Preserving_Facial_Expression_Manipulation_CVPR_2024_paper.pdf) Tianshui, Chen and Jianman, Lin and Zhijing, Yang and Chunmei, Qing and Liang, Lin [[code]](https://github.com/jianmanlincjx/ASCCL) CVPR Highlight
* [EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_EMAGE_Towards_Unified_Holistic_Co-Speech_Gesture_Generation_via_Expressive_Masked_CVPR_2024_paper.pdf)  Haiyang, Liu and Zihao, Zhu and Giorgio, Becherini and Yichen, Peng and Mingyang, Su and You, Zhou and Xuefei, Zhe and Naoya, Iwamoto and Bo, Zheng and Michael J. Black [[code]](https://github.com/PantoMatrix/PantoMatrix/tree/main/scripts/EMAGE_2024)
* [Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_Weakly-Supervised_Emotion_Transition_Learning_for_Diverse_3D_Co-speech_Gesture_Generation_CVPR_2024_paper.pdf) Xingqun, Qi and Jiahao, Pan and Peng, Li and Ruibin, Yuan and Xiaowei, Chi and Mengfei, Li and Wenhan, Luo and Wei, Xue and Shanghang, Zhang and Qifeng, Liu and Yike, Guo

## Audio

* [Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos](https://openaccess.thecvf.com/content/CVPR2024/papers/Majumder_Learning_Spatial_Features_from_Audio-Visual_Correspondence_in_Egocentric_Videos_CVPR_2024_paper.pdf) Sagnik, Majumder and Ziad, Al-Halah and Kristen, Grauman
* [QDFormer: Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_QDFormer_Towards_Robust_Audiovisual_Segmentation_in_Complex_Environments_with_Quantization-based_CVPR_2024_paper.pdf) Xiang, Li and Jinglu, Wang and Xiaohao, Xu and Xiulian, Peng and Rita, Singh and Yan, Lu and Bhiksha, Raj
* [UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio Video Point Cloud Time-Series and Image Recognition](https://openaccess.thecvf.com/content/CVPR2024/papers/Ding_UniRepLKNet_A_Universal_Perception_Large-Kernel_ConvNet_for_Audio_Video_Point_CVPR_2024_paper.pdf) Ding, Xiaohan and Zhang, Yiyuan and Ge, Yixiao and Zhao, Sijie and Song, Lin and Yue, Xiangyu and Shan, Ying [[code]](https://github.com/AILab-CVC/UniRepLKNet)
* [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action](https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.pdf) Jiasen, Lu and Christopher, Clark and Sangho, Lee and Zichen, Zhang and Savya, Khosla and Ryan, Marten and Derek, Hoiem and Aniruddha, Kembhavi [[code]](https://github.com/allenai/unified-io-2)
* [CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_CrossMAE_Cross-Modality_Masked_Autoencoders_for_Region-Aware_Audio-Visual_Pre-Training_CVPR_2024_paper.pdf) Yuxin, Guo and Siyang, Sun and Shuailei, Ma and Kecheng, Zheng and Xiaoyi, Bao and Shijie, Ma and Wei, Zou and Yun, Zheng
* [Looking Similar Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning](https://openaccess.thecvf.com/content/CVPR2024/papers/Singh_Looking_Similar_Sounding_Different_Leveraging_Counterfactual_Cross-Modal_Pairs_for_Audiovisual_CVPR_2024_paper.pdf) Nikhil, Singh and Chih-Wei, Wu and Iroro, Orife and Mahdi, Kalayeh
* [Unraveling Instance Associations: A Closer Look for Audio-Visual Segmentation](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Unraveling_Instance_Associations_A_Closer_Look_for_Audio-Visual_Segmentation_CVPR_2024_paper.pdf) Yuanhong, Chen and Yuyuan, Liu and Hu, Wang and Fengbei, Liu and Chong, Wang and Helen, Frazer and Gustavo, Carneiro
* [RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf) Zeyuan, Yang and Jiageng, Liu and Peihao, Chen and Anoop, Cherian and Tim K. Marks and Jonathan Le Roux and Chuang, Gan
* [DiVAS: Video and Audio Synchronization with Dynamic Frame Rates](https://openaccess.thecvf.com/content/CVPR2024/papers/Fernandez-Labrador_DiVAS_Video_and_Audio_Synchronization_with_Dynamic_Frame_Rates_CVPR_2024_paper.pdf) Clara Fernandez-Labrador and Mertcan, Ak√ßay and Eitan, Abecassis and Joan, Massich and Christopher, Schroers
* [Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners](https://openaccess.thecvf.com/content/CVPR2024/papers/Xing_Seeing_and_Hearing_Open-domain_Visual-Audio_Generation_with_Diffusion_Latent_Aligners_CVPR_2024_paper.pdf) Xing, Yazhou and He, Yingqing and Tian, Zeyue and Wang, Xintao and Chen, Qifeng [[code]](https://github.com/yzxing87/Seeing-and-Hearing)
* [AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection](https://openaccess.thecvf.com/content/CVPR2024/papers/Oorloff_AVFF_Audio-Visual_Feature_Fusion_for_Video_Deepfake_Detection_CVPR_2024_paper.pdf) Trevine, Oorloff and Surya, Koppisetti and Nicol√≤, Bonettini and Divyaraj, Solanki and Ben, Colman and Yaser, Yacoob and Ali, Shahriyari and Gaurav, Bharaj
* [AV-RIR: Audio-Visual Room Impulse Response Estimation](https://openaccess.thecvf.com/content/CVPR2024/papers/Ratnarajah_AV-RIR_Audio-Visual_Room_Impulse_Response_Estimation_CVPR_2024_paper.pdf) Anton, Ratnarajah and Sreyan, Ghosh and Sonal, Kumar and Purva, Chiniya and Dinesh, Manocha [[code]](https://github.com/anton-jeran/AV-RIR)
* [DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiong_DiffSal_Joint_Audio_and_Video_Learning_for_Diffusion_Saliency_Prediction_CVPR_2024_paper.pdf) Junwen, Xiong and Peng, Zhang and Tao, You and Chuanyue, Li and Wei, Huang and Yufei, Zha [[code]](https://github.com/junwenxiong/diff_sal)
* [Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_Unveiling_the_Power_of_Audio-Visual_Early_Fusion_Transformers_with_Dense_CVPR_2024_paper.pdf) Shentong, Mo and Pedro, Morgado [[code]](https://github.com/stoneMo/DeepAVFusion)
* [Weakly-Supervised Audio-Visual Video Parsing with Prototype-based Pseudo-Labeling](https://openaccess.thecvf.com/content/CVPR2024/papers/Rachavarapu_Weakly-Supervised_Audio-Visual_Video_Parsing_with_Prototype-based_Pseudo-Labeling_CVPR_2024_paper.pdf) Kranthi Kumar Rachavarapu and Kalyan, Ramakrishnan and Rajagopalan A. N.
* [The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective](https://openaccess.thecvf.com/content/CVPR2024/papers/Jia_The_Audio-Visual_Conversational_Graph_From_an_Egocentric-Exocentric_Perspective_CVPR_2024_paper.pdf) Wenqi, Jia and Miao, Liu and Hao, Jiang and Ishwarya, Ananthabhotla and James M. Rehg and Vamsi Krishna Ithapu and Ruohan, Gao
* [Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Cooperation_Does_Matter_Exploring_Multi-Order_Bilateral_Relations_for_Audio-Visual_Segmentation_CVPR_2024_paper.pdf) Qi, Yang and Xing, Nie and Tong, Li and Pengfei, Gao and Ying, Guo and Cheng, Zhen and Pengfei, Yan and Shiming, Xiang [[code]](https://github.com/yannqi/COMBO-AVS) CVPR Highlight
* [FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Aneja_FaceTalk_Audio-Driven_Motion_Diffusion_for_Neural_Parametric_Head_Models_CVPR_2024_paper.pdf) Shivangi, Aneja and Justus, Thies and Angela, Dai and Matthias, Nie√üner [[code]](https://github.com/shivangi-aneja/FaceTalk)
* [Audio-Visual Segmentation via Unlabeled Frame Exploitation](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Audio-Visual_Segmentation_via_Unlabeled_Frame_Exploitation_CVPR_2024_paper.pdf) Jinxiang, Liu and Yikun, Liu and Fei, Zhang and Chen, Ju and Ya, Zhang and Yanfeng, Wang
* [Cyclic Learning for Binaural Audio Generation and Localization](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Cyclic_Learning_for_Binaural_Audio_Generation_and_Localization_CVPR_2024_paper.pdf) Zhaojian, Li and Bin, Zhao and Yuan, Yuan
* [From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations](https://openaccess.thecvf.com/content/CVPR2024/papers/Ng_From_Audio_to_Photoreal_Embodiment_Synthesizing_Humans_in_Conversations_CVPR_2024_paper.pdf) Evonne, Ng and Javier, Romero and Timur, Bagautdinov and Shaojie, Bai and Trevor, Darrell and Angjoo, Kanazawa and Alexander, Richard [[code]](https://github.com/facebookresearch/audio2photoreal/)
* [FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_FaceChain-ImagineID_Freely_Crafting_High-Fidelity_Diverse_Talking_Faces_from_Disentangled_Audio_CVPR_2024_paper.pdf) Chao, Xu and Yang, Liu and Jiazheng, Xing and Weida, Wang and Mingze, Sun and Jun, Dan and Tianxin, Huang and Siyuan, Li and Zhi-Qi, Cheng and Ying, Tai and Baigui, Sun [[code]](https://github.com/modelscope/facechain)
* [Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Benchmarking_Audio_Visual_Segmentation_for_Long-Untrimmed_Videos_CVPR_2024_paper.pdf) Chen, Liu and Peike Patrick Li and Qingtao, Yu and Hongwei, Sheng and Dadong, Wang and Lincheng, Li and Xin, Yu
* [Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Real_Acoustic_Fields_An_Audio-Visual_Room_Acoustics_Dataset_and_Benchmark_CVPR_2024_paper.pdf) Ziyang, Chen and Israel D. Gebru and Christian, Richardt and Anurag, Kumar and William, Laney and Andrew, Owens and Alexander, Richard [[code]](https://github.com/facebookresearch/real-acoustic-fields/) CVPR Highlight
* [TIM: A Time Interval Machine for Audio-Visual Action Recognition](https://openaccess.thecvf.com/content/CVPR2024/papers/Chalk_TIM_A_Time_Interval_Machine_for_Audio-Visual_Action_Recognition_CVPR_2024_paper.pdf) Jacob, Chalk and Jaesung, Huh and Evangelos, Kazakos and Andrew, Zisserman and Dima, Damen [[code]](https://github.com/JacobChalk/TIM)
* [Separating the "Chirp" from the "Chat": Self-supervised Visual Grounding of Sound and Language](https://openaccess.thecvf.com/content/CVPR2024/papers/Hamilton_Separating_the_Chirp_from_the_Chat_Self-supervised_Visual_Grounding_of_CVPR_2024_paper.pdf) Mark, Hamilton and Andrew, Zisserman and John R. Hershey and William T. Freeman [[Code]](https://github.com/mhamilton723/DenseAV)
* [T-VSL: Text-Guided Visual Sound Source Localization in Mixtures](https://openaccess.thecvf.com/content/CVPR2024/papers/Mahmud_T-VSL_Text-Guided_Visual_Sound_Source_Localization_in_Mixtures_CVPR_2024_paper.pdf) Tanvir, Mahmud and Yapeng, Tian and Diana, Marculescu [[code]](https://github.com/enyac-group/T-VSL/tree/main)
* [Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge](https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Learning_to_Visually_Localize_Sound_Sources_from_Mixtures_without_Prior_CVPR_2024_paper.pdf) Dongjin, Kim and Sung Jin Um and Sangmin, Lee and Jung Uk Kim [[code]](https://github.com/VisualAIKHU/NoPrior_MultiSSL)
* [SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SoundingActions_Learning_How_Actions_Sound_from_Narrated_Egocentric_Videos_CVPR_2024_paper.pdf) Changan, Chen and Kumar, Ashutosh and Rohit Girdhar and David, Harwath and Kristen, Grauman
* [SonicVisionLM: Playing Sound with Vision Language Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_SonicVisionLM_Playing_Sound_with_Vision_Language_Models_CVPR_2024_paper.pdf) Zhifeng, Xie and Shengye, Yu and Qile, He and Mengtian, Li [[code]](https://github.com/Yusiissy/SonicVisionLM)
* [CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_CustomListener_Text-guided_Responsive_Interaction_for_User-friendly_Listening_Head_Generation_CVPR_2024_paper.pdf) Xi, Liu and Ying, Guo and Cheng, Zhen and Tong, Li and Yingying, Ao and Pengfei, Yan

## Music

* [Diff-BGM: A Diffusion Model for Video Background Music Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Diff-BGM_A_Diffusion_Model_for_Video_Background_Music_Generation_CVPR_2024_paper.pdf) Sizhe, Li and Yiming, Qin and Minghang, Zheng Xin Jin and Yang, Liu [[code]](https://github.com/sizhelee/Diff-BGM)
* [MuseChat: A Conversational Music Recommendation System for Videos](https://openaccess.thecvf.com/content/CVPR2024/papers/Dong_MuseChat_A_Conversational_Music_Recommendation_System_for_Videos_CVPR_2024_paper.pdf) Zhikang, Dong and Xiulong, Liu and Bin, Chen and Pawel, Polak and Peng, Zhang [[code]](https://github.com/Dongzhikang/MuseChat-dataset) CVPR Highlight
* [MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Chowdhury_MeLFusion_Synthesizing_Music_from_Image_and_Language_Cues_using_Diffusion_CVPR_2024_paper.pdf) Sanjoy, Chowdhury and Sayan, Nag and K J Joseph and Balaji Vasan Srinivasan and Dinesh, Manocha [[code]](https://github.com/schowdhury671/melfusion/tree/main) CVPR Highlight
* [DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DanceCamera3D_3D_Camera_Movement_Synthesis_with_Music_and_Dance_CVPR_2024_paper.pdf) Zixuan, Wang and Jia, Jia and Shikun, Sun and Haozhe, Wu and Rong, Han and Zhenyu, Li and Di, Tang and Jiaqing, Zhou and Jiebo, Luo [[code]](https://github.com/Carmenw1203/DanceCamera3D-Official)

## Contribute To the List
Please feel free to submit issues or email (xl1995@uw.edu) to add/modify links or correct wrong information.
