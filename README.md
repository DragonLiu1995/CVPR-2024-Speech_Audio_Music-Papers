# CVPR-2024-Speech_Audio_Music-Papers
A curated collections of papers related to speech, audio and music in CVPR 2024.

## üóûÔ∏è papers and posters

# Speech

* [Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion](https://openaccess.thecvf.com/content/CVPR2024/papers/Chhatre_Emotional_Speech-driven_3D_Body_Animation_via_Disentangled_Latent_Diffusion_CVPR_2024_paper.pdf) Chhatre, Kiran and Danƒõƒçek, Radek and Athanasiou, Nikos and Becherini, Giorgio and Peters, Christopher and Black, Michael J. and Bolkart, Timo [[code]](https://github.com/kiranchhatre/amuse)
* [ES¬≥: Evolving Self-Supervised Learning of Robust Audio-Visual Speech Representations](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_ES3_Evolving_Self-Supervised_Learning_of_Robust_Audio-Visual_Speech_Representations_CVPR_2024_paper.pdf) Yuanhang, Zhang and Shuang, Yang and Shiguang, Shan and Xilin, Chen
* [Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks Methods and Applications](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Probabilistic_Speech-Driven_3D_Facial_Motion_Synthesis_New_Benchmarks_Methods_and_CVPR_2024_paper.pdf) Karren D. Yang and Anurag, Ranjan and Jen-Hao Rick Chang and Raviteja, Vemulapalli and Oncel, Tuzel
* [DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_DiffSHEG_A_Diffusion-Based_Approach_for_Real-Time_Speech-driven_Holistic_3D_Expression_CVPR_2024_paper.pdf) Junming Chen, Yunfei Liu, Jianan, Wang and Ailing, Zeng and Yu, Li and Qifeng, Chen [[code]](https://github.com/JeremyCJM/DiffSHEG)
* [Faces that Speak: Jointly Synthesising Talking Face and Speech from Text](https://openaccess.thecvf.com/content/CVPR2024/papers/Jang_Faces_that_Speak_Jointly_Synthesising_Talking_Face_and_Speech_from_CVPR_2024_paper.pdf)  Youngjoon, Jang and Ji-Hoon, Kim and Junseok, Ahn and Doyeop, Kwak and Hong-Sun, Yang and Yoon-Cheol, Ju and Il-Hwan, Kim and Byeong-Yeol, Kim and Joon Son Chung
* [Towards Variable and Coordinated Holistic Co-Speech Motion Generation]([Towards Variable and Coordinated Holistic Co-Speech Motion Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Towards_Variable_and_Coordinated_Holistic_Co-Speech_Motion_Generation_CVPR_2024_paper.pdf)) Yifei Liu, Qiong Cao, Yandong Wen, Huaiguang Jiang, Changxing Ding [[code]](https://github.com/feifeifeiliu/ProbTalk)
* [ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis](https://openaccess.thecvf.com/content/CVPR2024/papers/Mughal_ConvoFusion_Multi-Modal_Conversational_Diffusion_for_Co-Speech_Gesture_Synthesis_CVPR_2024_paper.pdf) Muhammad Hamza Mughal and Rishabh, Dabral and Ikhsanul, Habibie and Lucia Donatelli and Marc, Habermann and Christian Theobalt [[code]](https://github.com/m-hamza-mughal/convofusion)
* [A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition](https://openaccess.thecvf.com/content/CVPR2024/papers/Dai_A_Study_of_Dropout-Induced_Modality_Bias_on_Robustness_to_Missing_CVPR_2024_paper.pdf) Yusheng, Dai and Hang, Chen and Jun, Du and Xiaofei, Ding and Ning, Ding and Feijun, Jiang and Chin-Hui, Lee [[code]](https://github.com/dalision/ModalBiasAVSR)
* [AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation](https://openaccess.thecvf.com/content/CVPR2024/papers/Choi_AV2AV_Direct_Audio-Visual_Speech_to_Audio-Visual_Speech_Translation_with_Unified_CVPR_2024_paper.pdf) Jeongsoo, Choi and Se Jin Park and Minsu, Kim and Yong Man Ro [[code]](https://github.com/choijeongsoo/av2av), CVPR Highlight
* [Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model](https://openaccess.thecvf.com/content/CVPR2024/papers/He_Co-Speech_Gesture_Video_Generation_via_Motion-Decoupled_Diffusion_Model_CVPR_2024_paper.pdf) He, Xu and Huang, Qiaochu and Zhang, Zhensong and Lin, Zhiwei and Wu, Zhiyong and Yang, Sicheng and Li, Minglei and Chen, Zhiyi and Xu, Songcen and Wu, Xiaofei [[code]](https://github.com/thuhcsi/S2G-MDDiffusion)
* [Learning Adaptive Spatial Coherent Correlations for Speech-Preserving Facial Expression Manipulation](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Learning_Adaptive_Spatial_Coherent_Correlations_for_Speech-Preserving_Facial_Expression_Manipulation_CVPR_2024_paper.pdf) Tianshui, Chen and Jianman, Lin and Zhijing, Yang and Chunmei, Qing and Liang, Lin [[code]](https://github.com/jianmanlincjx/ASCCL) CVPR Highlight
* [EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_EMAGE_Towards_Unified_Holistic_Co-Speech_Gesture_Generation_via_Expressive_Masked_CVPR_2024_paper.pdf)  Haiyang, Liu and Zihao, Zhu and Giorgio, Becherini and Yichen, Peng and Mingyang, Su and You, Zhou and Xuefei, Zhe and Naoya, Iwamoto and Bo, Zheng and Michael J. Black [[code]](https://github.com/PantoMatrix/PantoMatrix/tree/main/scripts/EMAGE_2024)
* [Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_Weakly-Supervised_Emotion_Transition_Learning_for_Diverse_3D_Co-speech_Gesture_Generation_CVPR_2024_paper.pdf) Xingqun, Qi and Jiahao, Pan and Peng, Li and Ruibin, Yuan and Xiaowei, Chi and Mengfei, Li and Wenhan, Luo and Wei, Xue and Shanghang, Zhang and Qifeng, Liu and Yike, Guo
